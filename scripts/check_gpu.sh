#!/bin/bash

# üéÆ GPU Environment Checker Script
#
# Comprehensive GPU environment validation for the LLaMA-3 8B Compression System.
# Checks CUDA availability, memory, PyTorch integration, and model requirements.
#
# Prerequisites:
# - NVIDIA GPU with CUDA support
# - PyTorch with CUDA support installed
#
# Usage:
#   ./scripts/check_gpu.sh
#
# What this script does:
# 1. Checks NVIDIA driver and CUDA installation
# 2. Validates PyTorch CUDA integration
# 3. Tests GPU memory and capabilities
# 4. Validates model path accessibility
# 5. Provides recommendations

set -e  # Exit on any error

echo "üéÆ GPU Environment Checker for LLaMA-3 8B Compression System..."
echo "============================================================="
echo "üìç Host: $(hostname)"
echo "üìÖ Date: $(date)"
echo ""

# Check NVIDIA driver
echo "üîç Checking NVIDIA driver..."
if command -v nvidia-smi &> /dev/null; then
    echo "‚úì nvidia-smi available"
    nvidia-smi --query-gpu=name,driver_version,memory.total,memory.used,memory.free --format=csv,noheader,nounits | while IFS=, read name driver memory_total memory_used memory_free; do
        echo "  GPU: $name"
        echo "  Driver: $driver"
        echo "  Memory: ${memory_used}MB used / ${memory_total}MB total (${memory_free}MB free)"
    done
else
    echo "‚ùå nvidia-smi not available"
    echo "   Make sure you're on a GPU node and NVIDIA drivers are installed"
    exit 1
fi

# Check CUDA availability
echo ""
echo "üîç Checking CUDA installation..."
if command -v nvcc &> /dev/null; then
    cuda_version=$(nvcc --version | grep "release" | sed 's/.*release \([0-9.]*\).*/\1/')
    echo "‚úì CUDA version: $cuda_version"
else
    echo "‚ö†Ô∏è  nvcc not found (CUDA toolkit may not be installed)"
fi

# Check PyTorch CUDA integration
echo ""
echo "üîç Checking PyTorch CUDA integration..."
python3 -c "
import sys
try:
    import torch
    print(f'‚úì PyTorch version: {torch.__version__}')
    
    if torch.cuda.is_available():
        print(f'‚úì CUDA available in PyTorch')
        print(f'  CUDA version: {torch.version.cuda}')
        print(f'  cuDNN version: {torch.backends.cudnn.version()}')
        print(f'  Number of GPUs: {torch.cuda.device_count()}')
        
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            memory_gb = props.total_memory / (1024**3)
            print(f'  GPU {i}: {props.name} ({memory_gb:.1f} GB)')
            
        # Test GPU memory allocation
        try:
            test_tensor = torch.randn(1000, 1000, device='cuda')
            print(f'‚úì GPU memory allocation test: PASSED')
            del test_tensor
            torch.cuda.empty_cache()
        except Exception as e:
            print(f'‚ùå GPU memory allocation test: FAILED ({e})')
            
    else:
        print('‚ùå CUDA not available in PyTorch')
        print('   This may be due to:')
        print('   - PyTorch CPU-only installation')
        print('   - CUDA version mismatch')
        print('   - Missing CUDA libraries')
        sys.exit(1)
        
except ImportError:
    print('‚ùå PyTorch not installed')
    print('   Install with: pip install torch --index-url https://download.pytorch.org/whl/cu118')
    sys.exit(1)
"

# Check memory requirements for LLaMA-3 8B
echo ""
echo "üîç Checking LLaMA-3 8B memory requirements..."
python3 -c "
import torch
if torch.cuda.is_available():
    total_memory = torch.cuda.get_device_properties(0).total_memory
    memory_gb = total_memory / (1024**3)
    
    # LLaMA-3 8B requirements
    required_memory = 16  # GB
    recommended_memory = 24  # GB
    
    print(f'  Available GPU memory: {memory_gb:.1f} GB')
    print(f'  Required for LLaMA-3 8B: {required_memory} GB')
    print(f'  Recommended: {recommended_memory} GB')
    
    if memory_gb >= recommended_memory:
        print('‚úì Memory: EXCELLENT (recommended or above)')
    elif memory_gb >= required_memory:
        print('‚úì Memory: SUFFICIENT (minimum requirements met)')
    else:
        print(f'‚ö†Ô∏è  Memory: INSUFFICIENT (need at least {required_memory} GB)')
        print('   Consider using smaller batch sizes or model sharding')
else:
    print('‚ùå Cannot check memory requirements (no CUDA)')
"

# Check model path accessibility
echo ""
echo "üîç Checking LLaMA-3 8B model accessibility..."
model_path="/mnt/vstor/CSE_ECSE_GXD234/Meta-Llama-3-8B-Instruct"
if [ -d "$model_path" ]; then
    echo "‚úì Model path accessible: $model_path"
    
    # Check key model files
    if [ -f "$model_path/config.json" ]; then
        echo "‚úì config.json found"
    else
        echo "‚ö†Ô∏è  config.json not found"
    fi
    
    if ls "$model_path"/*.safetensors >/dev/null 2>&1; then
        safetensor_count=$(ls "$model_path"/*.safetensors | wc -l)
        echo "‚úì SafeTensor files found ($safetensor_count files)"
    elif ls "$model_path"/*.bin >/dev/null 2>&1; then
        bin_count=$(ls "$model_path"/*.bin | wc -l)
        echo "‚úì Bin files found ($bin_count files)"
    else
        echo "‚ö†Ô∏è  No model weight files found (.safetensors or .bin)"
    fi
    
    if [ -f "$model_path/tokenizer.json" ] || [ -f "$model_path/tokenizer.model" ]; then
        echo "‚úì Tokenizer files found"
    else
        echo "‚ö†Ô∏è  Tokenizer files not found"
    fi
    
else
    echo "‚ùå Model path not accessible: $model_path"
    echo "   Make sure the model is available and mounted correctly"
fi

# Check transformers library
echo ""
echo "üîç Checking transformers library..."
python3 -c "
try:
    import transformers
    print(f'‚úì Transformers version: {transformers.__version__}')
    
    # Test model loading capability
    from transformers import AutoConfig
    try:
        config = AutoConfig.from_pretrained('$model_path', trust_remote_code=True)
        print(f'‚úì Model config loadable: {config.model_type}')
    except Exception as e:
        print(f'‚ö†Ô∏è  Model config loading issues: {e}')
        
except ImportError:
    print('‚ùå Transformers not installed')
    print('   Install with: pip install transformers>=4.30.0')
"

# Performance recommendations
echo ""
echo "üí° Performance Recommendations:"
echo "================================"

# Check if we're in a SLURM environment
if [ ! -z "$SLURM_JOB_ID" ]; then
    echo "‚úì Running in SLURM environment (Job ID: $SLURM_JOB_ID)"
    echo "  ‚Ä¢ Allocated GPUs: ${CUDA_VISIBLE_DEVICES:-'all available'}"
else
    echo "‚Ä¢ Consider using SLURM for resource management"
fi

# Check CPU count
cpu_count=$(nproc)
echo "‚Ä¢ CPU cores available: $cpu_count"
if [ $cpu_count -ge 16 ]; then
    echo "  ‚úì Sufficient CPU cores for data loading"
else
    echo "  ‚ö†Ô∏è  Consider requesting more CPU cores for optimal data loading"
fi

# Memory recommendations
echo "‚Ä¢ For optimal performance:"
echo "  - Use mixed precision training (fp16)"
echo "  - Enable gradient checkpointing if memory limited"
echo "  - Use DeepSpeed for larger models"

echo ""
echo "üéâ GPU Environment Check Complete!"
echo ""
echo "üìã Next steps:"
echo "  ‚Ä¢ Run benchmark: python tests/integration/run_comprehensive_test.py"
echo "  ‚Ä¢ Start development: ./scripts/dev_setup.sh"
echo "  ‚Ä¢ Monitor GPU usage: watch -n 1 nvidia-smi"